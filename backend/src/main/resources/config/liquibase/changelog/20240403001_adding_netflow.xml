<?xml version="1.0" encoding="utf-8"?>
<databaseChangeLog
        xmlns="http://www.liquibase.org/xml/ns/dbchangelog"
        xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xsi:schemaLocation="http://www.liquibase.org/xml/ns/dbchangelog http://www.liquibase.org/xml/ns/dbchangelog/dbchangelog-3.5.xsd">

    <changeSet id="20240403001" author="Manuel">

        <sql dbms="postgresql" splitStatements="true" stripComments="true">
            <![CDATA[
                INSERT INTO utm_logstash_filter (id, logstash_filter, filter_name, filter_group_id, system_owner, module_name, is_active, filter_version)
                VALUES (1523, 'filter {
 #Netflow filter bassed on https://www.cisco.com/en/US/technologies/tk648/tk362/technologies_white_paper09186a00800a3db9.html (february 2022)
 #and http://www.iana.org/assignments/ipfix/ipfix.xhtml (february 2022)

#Filter version 2.1.0

  split {
    field => "message"
    terminator => "<utm-log-separator>"
  }

  #Looking for datasource generated by an agent and parse original message
  if [message]=~/\[utm_stack_agent_ds=(.+)\]-(.+)/ {
    grok {
      match => {
        "message" => [ "\[utm_stack_agent_ds=%{DATA:dataSource}\]-%{GREEDYDATA:original_log_message}" ]
      }
    }
  }
  if [original_log_message] {
    mutate {
      update => { "message" => "%{[original_log_message]}" }
    }
  }

  if ![dataType] {
# The log destination is already identified by the agent so, don''t need an entry point
#......................................................................#
  #First, replace whitespaces with default string after = to avoid kv issues, example:
  #device_id= date=2021-08-18, generates -> device_id="date=2021-08-18"
  #and should generate two fields: device_id and date
    mutate {
      gsub => [
        "message", "= ", "=X0X "
      ]
    }
    mutate {
      gsub => [
        "message", ''=""'', "=X0X "
      ]
    }

  #......................................................................#
  #Using grok to parse priority if present
    grok {
      match => {
        "message" => [
          "(<%{NUMBER:priority}>)?%{GREEDYDATA:message_rest}"
        ]
      }
    }

  #......................................................................#
  #Using the kv filter with default config, usefull in key-value logs
    if [message_rest] {
      kv {
        source => "message_rest"
        allow_duplicate_values => false
        target => "kv_field"
      }
    }
  #......................................................................#
  #Remove fields that have issues with kv filter
    mutate {
      remove_field => ["[kv_field][msg]"]
    }
  #......................................................................#
  #Using grok to parse kv issued fields
    if [message_rest] {
      grok {
        match => {
          "message_rest" => [
            "%{GREEDYDATA} msg=%{QUOTEDSTRING:msg}%{GREEDYDATA}"
          ]
        }
      }
  #......................................................................#
  #Remove double quotation and add to kv_field
      mutate {
        gsub => ["[msg]", ''"'', ""]
      }
    }
    mutate {
      rename => { "msg" => "[kv_field][msg]" }
      rename => { "message" => "[kv_field][message]" }
    }
  #......................................................................#
  #Generating dataSource field required by CorrelationRulesEngine
    if (![dataSource]){
     mutate {
        add_field => { "dataSource" => "%{host}" }
     }
    }

  #......................................................................#
  #Generating dataType field required by CorrelationRulesEngine
    mutate {
      add_field => {
        "dataType" => "netflow"
      }
    }
  #......................................................................#
  #Cleaning message field
    mutate {
        gsub => ["[kv_field][message]", "X0X", ""]
    }
  #......................................................................#
  # Generating fields required by correlation rules
      mutate {
        #IPFIX fields
        rename => {"[kv_field][srcPort]" => "[kv_field][src_port]"}
        rename => {"[kv_field][srcIp]" => "[kv_field][src_ip]"}
        rename => {"[kv_field][dstPort]" => "[kv_field][dest_port]"}
        rename => {"[kv_field][dstIp]" => "[kv_field][dest_ip]"}
        rename => {"[kv_field][proto]" => "[kv_field][proto]"}
      }

  #......................................................................#
  #Set null the fields with de X0X value (default string for null), and replace simple and double quotation
  #also generate logx tree structure dynamically
    if [kv_field] {
      ruby {
        code => ''
          event.get("[kv_field]").each do |k, v|
          if (v == "X0X")
            event.set("[logx][netflow][#{k}]",nil)
          elsif !(v.kind_of?(Array))
            new_v = v.to_s.gsub(/\"/, "")
            new_v = new_v.gsub(/\''/, "")
            event.set("[logx][netflow][#{k}]",new_v)
          else
            event.set("[logx][netflow][#{k}]",v)
          end
          end
        ''
      }
    }


    #Finally, remove unnecessary fields
    mutate {
        remove_field => ["@version","path","message_rest","tags","kv_field","original_log_message","headers"]
    }
  }
}', 'netflow', null, true, 'NETFLOW', false, '2.1.0');
            ]]>
        </sql>
        <sql dbms="postgresql" splitStatements="true" stripComments="true">
            <![CDATA[
                    INSERT INTO utm_logstash_pipeline (id, pipeline_id, pipeline_name, parent_pipeline, pipeline_status, module_name, system_owner, pipeline_description, pipeline_internal, events_in, events_filtered, events_out, reloads_successes, reloads_failures, reloads_last_failure_timestamp, reloads_last_error, reloads_last_success_timestamp)
                    VALUES (48, 'netflow', 'Netflow', null, 'up', 'NETFLOW', true, null, false, 0, 0, 0, 0, 0, null, null, null);

                    INSERT INTO utm_group_logstash_pipeline_filters (filter_id, pipeline_id, relation)
                    VALUES (1523, 48, 'PIPELINE_FILTER');

                    INSERT INTO utm_logstash_input (id, pipeline_id, input_pretty_name, input_plugin, input_with_ssl, system_owner)
                    VALUES (64, 48, 'HTTP', 'http', false, true);

                    INSERT INTO utm_logstash_input_configuration (id, input_id, conf_key, conf_value, conf_type, conf_required, conf_validation_regex, system_owner)
                    VALUES (64, 64, 'http_port', '10044', 'port', true, '^((6553[0-5])|(655[0-2][0-9])|(65[0-4][0-9]{2})|(6[0-4][0-9]{3})|([1-5][0-9]{4})|([0-5]{0,5})|([0-9]{1,4}))$', true);

                    INSERT INTO utm_index_pattern (id, pattern, pattern_module, pattern_system, is_active)
                    VALUES (64,'log-netflow-*', 'NETFLOW', true, true);

                    INSERT INTO utm_data_source_config (data_type, data_type_name, system_owner, included)
                    VALUES ( 'netflow', 'Netflow', true, true);

                    INSERT INTO utm_menu (id, name, url, parent_id, type, dashboard_id, position, menu_active, menu_action, menu_icon, module_name_short)
                    VALUES (262, 'Netflow', 'discover/log-analyzer?patternId=64&indexPattern=log-netflow-*', 200, 1, null, 61, false, false, null, 'NETFLOW');

                    INSERT INTO utm_menu_authority (menu_id, authority_name)
                    VALUES ( 262, 'ROLE_USER');

                    INSERT INTO utm_menu_authority (menu_id, authority_name)
                    VALUES ( 262, 'ROLE_ADMIN');


            ]]>


        </sql>
    </changeSet>
</databaseChangeLog>
